# from transformers import AutoTokenizer
# import re
# from tokenizer.token import Token
#
# def get_token_pattern() -> str:
#     pattern = r"""
#         (?:[A-Za-z]\.)+(?=\W|$)            # Abbreviations like U.S.A., B.
#         |
#         \w+(?:['-]\w+)*(?:'s|')?(?!\w)     # Words with optional internal apostrophes/hyphens and possessives
#         |
#         [^\w\s]                            # Punctuation
#         |
#         \s+                                # Whitespace
#     """
#     return pattern
#
# tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", do_basic_tokenize=False)
#
# def tokenize_english_text(text: str) -> list[Token]:
#     tokens = []
#     pattern = get_token_pattern()
#     # Compile the pattern with verbose mode
#     regex = re.compile(pattern, re.VERBOSE)
#     for match in regex.finditer(text):
#         token_text = match.group()
#         start_pos = match.start()
#         tokens.append(Token(text=token_text, start=start_pos))
#     return tokens